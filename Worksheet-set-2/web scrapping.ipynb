{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                            Web Scraping worksheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)   Write a python program to display all the header tags from‘en.wikipedia.org/wiki/Main_Page’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import Beautifulsoap\n",
    "\n",
    "url = requests.get('en.wikipedia.org/wiki/Main_Page')\n",
    "soap = Beautifulsoap(url.text , 'html.parser')\n",
    "story = soap.find_all(['h1','h2','h3'])\n",
    "for i in story:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. Name, IMDB rating, Year of \n",
    "release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoap\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "\n",
    "url = 'http://www.imdb.com/chart/top'\n",
    "response = requests.get(url)\n",
    "Soap = BeautifulSoap(response.text,'lxml')\n",
    "\n",
    "movie = Soap.select('td.titleColumn')\n",
    "link = [a.attrs.get('href') for a in Soap.select('td.titleColumn a') ]\n",
    "\n",
    "ratings = [b.attrs.get('data-value')\n",
    "              for b in Soap.select('td.posterColumn span[name=ir]')]\n",
    "\n",
    "for index in range(100, len(movie)):\n",
    "    movie_string = movie[index].get_text()\n",
    "    movie = (' ' .join(movie_string.split()).replace(' . ', ''))\n",
    "    movie_title = movie[len(str(index))+1:-7]\n",
    "    year = re.serach('\\((.*?)\\)', movie_string).group(1)\n",
    "    place = movie[:len(str(index))-(len(movie))]\n",
    "    data = [[\"Movie_title\":movie_title],\n",
    "           [\"Year\": year],\n",
    "           [\"Place\": place], \n",
    "           [\"Rating\": ratings[index]],\n",
    "           [\"Link\": link[index]]]\n",
    "    \n",
    "    df = pd.DataFrame(data,colume,colume1,colume2 =['movie','year','rating'])\n",
    "    \n",
    "    df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. Name, IMDB rating, Year \n",
    "of release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoap\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "url = 'https://www.imdb.com/india/top-rated-indian-movies/'\n",
    "response = requests.get(url)\n",
    "Soap = BeautifulSoap(response.text,'lxml')\n",
    "\n",
    "movie = Soap.select('table.chart td.titleColumn')\n",
    "link = [a.attrs.get('href') for a in Soap.select('table.chart td.titleColumn a') ]\n",
    "\n",
    "ratings = [b.attrs.get('data-value')\n",
    "              for b in Soap.select('td.ratingColumn span[name=ir]')]\n",
    "\n",
    "list = []\n",
    "for index in range(100, len(movie)):\n",
    "    movie_string = movie[index].get_text()\n",
    "    movie = (' ' .join(movie_string.split()).replace(' . ', ''))\n",
    "    movie_title = movie[len(str(index))+1:-7]\n",
    "    year = re.serach('\\((.*?)\\)', movie_string).group(1)\n",
    "    place = movie[:len(str(index))-(len(movie))]\n",
    "    data = {\"movie_title\":movie_title,\n",
    "            \"year\": year,\n",
    "            \"place\": place\n",
    "            \"rating\": ratings[index],\n",
    "            \"link\": link[index]}\n",
    "    list.append(data)\n",
    "    \n",
    "    for movie in list :\n",
    "        print(movie['place'], '-', movie['movie_title'], '('+movie[' year ']')', movie['rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Write a python program to scrape cricket rankings from ‘www.icc-cricket.com’. You have toscrape:\n",
    "i) Top 10 ODI teams in men’s cricket along with the records for matches, points andrating.\n",
    "ii) Top 10 ODI Batsmen in men along with the records of their team and rating.\n",
    "iii) Top 10 ODI bowlers along with the records of their team andrating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top 10 ODI team in men cricket along with records for matches, point \n",
    "\n",
    "from bs4 import BeautifulSoap\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "url = 'https://www.icc-cricket.com/rankings/mens/team-rankings/odi'\n",
    "response = requests.get(url)\n",
    "Soap = BeautifulSoap(response.text,'lxml')\n",
    "\n",
    "Team = Soap.select('td.table-body__cell.ranking-table__team')\n",
    "link = [a.attrs.get('href') for a in Soap.select('td.table-body__cell.ranking-table__team') ]\n",
    "\n",
    "Rating = [b.attrs.get('data-value')\n",
    "              for b in Soap.select('td.rankings-block__banner td')]\n",
    "\n",
    "for index in range(10, len(Team)):\n",
    "    Team_string = team[index].get_text()\n",
    "    team = (' ' .join(Team_string.split()).replace(' . ', ''))\n",
    "    Team_name = Team[len(str(index))+1:-7]\n",
    "    Matches = re.serach('\\((.*?)\\)', Team_string).group(1)\n",
    "    Point = Team[:len(str(index))-(len(Team))]\n",
    "    data = [[\"team_name\":team_name],\n",
    "           [\"Matches\": Matches],\n",
    "           [\"Point\": Point], \n",
    "           [\"Rating\": ratings[index]],\n",
    "           [\"Link\": link[index]]]\n",
    "    \n",
    "    df = pd.DataFrame(data,colume,colume1,colume2,colume3=['Team_name','Matches','Point','rating',])\n",
    "    \n",
    "    df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 ODI batsmen in cricket along with records for matches\n",
    "\n",
    "url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting'\n",
    "response = requests.get(url)\n",
    "Soap = BeautifulSoap(response.text,'lxml')\n",
    "\n",
    "Batsmen = Soap.select('td.rankings-block__container .table-body__cell.name a ')\n",
    "link = [a.attrs.get('href') for a in Soap.select('td.rankings-block__container .table-body__cell.name a ') ]\n",
    "\n",
    "Rating = [b.attrs.get('data-value')\n",
    "              for b in Soap.select('td.rankings-block__container .table .rating')]\n",
    "\n",
    "for index in range(10, len(Batsmen)):\n",
    "    Batsmen_string = Batsmen[index].get_text()\n",
    "    Player = (' ' .join(Batsmen_string.split()).replace(' . ', ''))\n",
    "    Player_name = Batsmen[len(str(index))+1:-7]\n",
    "    Team = re.serach('\\((.*?)\\)', Batsmen_string).group(1)\n",
    "    data = [[\"Player_name\":Player_name],\n",
    "           [\"Team\": Team], \n",
    "           [\"Rating\": ratings[index]],\n",
    "           [\"Link\": link[index]]]\n",
    "    \n",
    "    df = pd.DataFrame(data,colume,colume1 =['Player_name','Team','Rating',])\n",
    "    \n",
    "    df \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 bowlers in cricket along with record for matches\n",
    "\n",
    "url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling'\n",
    "response = requests.get(url)\n",
    "Soap = BeautifulSoap(response.text,'lxml')\n",
    "\n",
    "Bowlers = Soap.select('td.rankings-block__container .table-body__cell.name a ')\n",
    "link = [a.attrs.get('href') for a in Soap.select('td.rankings-block__container .table-body__cell.name a ') ]\n",
    "\n",
    "Rating = [b.attrs.get('data-value')\n",
    "              for b in Soap.select('td.rankings-block__container .table .rating')]\n",
    "\n",
    "for index in range(10, len(Bowlers)):\n",
    "    Bowlers_string = Bowlers[index].get_text()\n",
    "    Player = (' ' .join(Bowlers_string.split()).replace(' . ', ''))\n",
    "    Player_name = Bowlers[len(str(index))+1:-7]\n",
    "    Team = re.serach('\\((.*?)\\)', Bowlers_string).group(1)\n",
    "    data = [[\"Player_name\":Player_name],\n",
    "           [\"Team\": Team], \n",
    "           [\"Rating\": ratings[index]],\n",
    "           [\"Link\": link[index]]]\n",
    "    \n",
    "    df = pd.DataFrame(data,colume,colume1 =['Player_name','Team','Rating',])\n",
    "    \n",
    "    df \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5)Write a python program to scrape cricket rankings from ‘www.icc-cricket.com’. You have toscrape:\n",
    "i) Top 10 ODI teams in women’s cricket along with the records for matches, points andrating.\n",
    "ii) Top 10 women’s ODI players along with the records of their team and rating.\n",
    "iii) Top 10 women’s ODI all-rounder along with the records of their team andrating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 ODI team in women cricket along with the record for matches\n",
    "\n",
    "url = 'https://www.icc-cricket.com/rankings/womens/team-rankings/odi'\n",
    "response = requests.get(url)\n",
    "Soap = BeautifulSoap(response.text,'lxml')\n",
    "\n",
    "Team = Soap.select('td.rankings-block__container .table-body__cell')\n",
    "link = [a.attrs.get('href') for a in Soap.select('td.rankings-block__container .table-body__cell ') ]\n",
    "\n",
    "Rating = [b.attrs.get('data-value')\n",
    "              for b in Soap.select('td.rankings-block__container .table .rating')]\n",
    "\n",
    "for index in range(10, len(Team)):\n",
    "    Team_string = Team[index].get_text()\n",
    "    Team = (' ' .join(Team_string.split()).replace(' . ', ''))\n",
    "    Team_name = Team[len(str(index))+1:-7]\n",
    "    rank = re.serach('\\((.*?)\\)', Bowlers_string).group(1)\n",
    "    data = [[\"Team_name\":Team_name],\n",
    "           [\"Team\": Team], \n",
    "           [\"Rank\": rank[index]],\n",
    "           [\"Link\": link[index]]]\n",
    "    \n",
    "    df = pd.DataFrame(data,colume,colume1 =['Team_name','Team','Rank',])\n",
    "    \n",
    "    df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 ODI players in women cricket along with the record for matches\n",
    "\n",
    "url = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi'\n",
    "response = requests.get(url)\n",
    "Soap = BeautifulSoap(response.text,'lxml')\n",
    "\n",
    "Batsmen = Soap.select('td.rankings-block__container .table-body__cell.name a ')\n",
    "link = [a.attrs.get('href') for a in Soap.select('td.rankings-block__container .table-body__cell.name a ') ]\n",
    "\n",
    "Rating = [b.attrs.get('data-value')\n",
    "              for b in Soap.select('td.rankings-block__container .table .rating')]\n",
    "\n",
    "for index in range(10, len(Batsmen)):\n",
    "    Batsmen_string = Batsmen[index].get_text()\n",
    "    Player = (' ' .join(Batsmen_string.split()).replace(' . ', ''))\n",
    "    Player_name = Batsmen[len(str(index))+1:-7]\n",
    "    Team = re.serach('\\((.*?)\\)', Batsmen_string).group(1)\n",
    "    data = [[\"Player_name\":Player_name],\n",
    "           [\"Team\": Team], \n",
    "           [\"Rating\": ratings[index]],\n",
    "           [\"Link\": link[index]]]\n",
    "    \n",
    "    df = pd.DataFrame(data,colume,colume1 =['Player_name','Team','Rating',])\n",
    "    \n",
    "    df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 womens ODI all-rounder along with the record for matches\n",
    "\n",
    "url = 'https://www.icc-cricket.comhttps://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder'\n",
    "response = requests.get(url)\n",
    "Soap = BeautifulSoap(response.text,'lxml')\n",
    "\n",
    "Batsmen = Soap.select('td.rankings-block__container .table-body__cell.name a ')\n",
    "link = [a.attrs.get('href') for a in Soap.select('td.rankings-block__container .table-body__cell.name a ') ]\n",
    "\n",
    "Rating = [b.attrs.get('data-value')\n",
    "              for b in Soap.select('td.rankings-block__container .table .rating')]\n",
    "\n",
    "for index in range(10, len(Batsmen)):\n",
    "    Batsmen_string = Batsmen[index].get_text()\n",
    "    Player = (' ' .join(Batsmen_string.split()).replace(' . ', ''))\n",
    "    Player_name = Batsmen[len(str(index))+1:-7]\n",
    "    Team = re.serach('\\((.*?)\\)', Batsmen_string).group(1)\n",
    "    data = [[\"Player_name\":Player_name],\n",
    "           [\"Team\": Team], \n",
    "           [\"Rating\": ratings[index]],\n",
    "           [\"Link\": link[index]]]\n",
    "    \n",
    "    df = pd.DataFrame(data,colume,colume1 =['Player_name','Team','Rating',])\n",
    "    \n",
    "    df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Write a python program to scrape details of all the mobile phones under Rs. 20,000 listed on Amazon.in. The \n",
    "scraped data should include Product Name, Price, Image URL and Average Rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    " \n",
    "def get_title(soup):\n",
    "     \n",
    "    try:\n",
    "        title = soup.find(\"span\", attrs={\"id\":'productTitle'})\n",
    " \n",
    "        title_value = title.string\n",
    " \n",
    "        title_string = title_value.strip()\n",
    " \n",
    " \n",
    "    except AttributeError:\n",
    "        title_string = \"\"   \n",
    " \n",
    "    return title_string\n",
    " \n",
    "def get_price(soup):\n",
    " \n",
    "    try:\n",
    "        price = soup.find(\"span\", attrs={'id':'priceblock_ourprice'}).string.strip()\n",
    " \n",
    "    except AttributeError:\n",
    "        price = \"\"  \n",
    " \n",
    "    return price\n",
    " \n",
    "def get_rating(soup):\n",
    " \n",
    "    try:\n",
    "        rating = soup.find(\"i\", attrs={'class':'a-icon a-icon-star a-star-4-5'}).string.strip()\n",
    "         \n",
    "    except AttributeError:\n",
    "         \n",
    "        try:\n",
    "            rating = soup.find(\"span\", attrs={'class':'a-icon-alt'}).string.strip()\n",
    "        except:\n",
    "            rating = \"\" \n",
    " \n",
    "    return rating\n",
    " \n",
    "def get_review_count(soup):\n",
    "    try:\n",
    "        review_count = soup.find(\"span\", attrs={'id':'acrCustomerReviewText'}).string.strip()\n",
    "         \n",
    "    except AttributeError:\n",
    "        review_count = \"\"   \n",
    " \n",
    "    return review_count\n",
    " \n",
    "def get_availability(soup):\n",
    "    try:\n",
    "        available = soup.find(\"div\", attrs={'id':'availability'})\n",
    "        available = available.find(\"span\").string.strip()\n",
    " \n",
    "    except AttributeError:\n",
    "        available = \"\"  \n",
    " \n",
    "    return available    \n",
    " \n",
    "if __name__ == '__main__':\n",
    " \n",
    "    HEADERS = ({'User-Agent':\n",
    "                'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',\n",
    "                'Accept-Language': 'en-US, en;q=0.5'})\n",
    " \n",
    "    URL = \"https://www.amazon.in/s?bbn=1389401031&rh=n%3A1389401031%2Cp_36%3A1318506031&dc&qid=1635181279&rnid=1318502031&ref=lp_1389401031_nr_p_36_3\"\n",
    " \n",
    "    webpage = requests.get(URL, headers=HEADERS)\n",
    " \n",
    "    soup = BeautifulSoup(webpage.content, \"lxml\")\n",
    " \n",
    "    print(\"Product Title =\", get_title(soup))\n",
    "    print(\"Product Price =\", get_price(soup))\n",
    "    print(\"Product Rating =\", get_rating(soup))\n",
    "    print(\"Number of Product Reviews =\", get_review_count(soup))\n",
    "    print(\"Availability =\", get_availability(soup))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Write a python program to scrape house details from mentioned url. It should include house title, location, \n",
    "area, emi and price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import ast\n",
    "import urllib.request\n",
    "from urllib.request import Request,urlopen\n",
    "\n",
    "url = input('https://www.nobroker.in/property/sale/bangalore/Electronic%20City?type=BHK4&searchParam=W3sibGF0IjoxMi44NDUyMTQ1LCJsb24iOjc3LjY2MDE2OTUsInBsYWNlSWQiOiJDaElKdy1GUWQ0cHNyanNSSGZkYXpnXzhYRW8iLCJwbGFjZU5hbWUiOiJFbGVjdHJvbmljIENpdHkifV0%3D&propertyAge=0&radius=2.0%22')\n",
    "\n",
    "req = Request(url,headers = {'user-Agent':'Mozilla/5,0'})\n",
    "\n",
    "webpage = urlopen(req).read()\n",
    "\n",
    "Soup = BeautifulSoup(webpage,'html.parser')\n",
    "html = Soup.prettify('utf-8')\n",
    "proparty_json ={}\n",
    "proparty_json['Detail_Broad']={}\n",
    "proparty_json['Address']={}\n",
    "\n",
    "for tittle in soup.find all('tittle'):\n",
    "    proparty_json['tittle'] = tittle.text.strip()\n",
    "    \n",
    "    break\n",
    "    \n",
    "for meta in soup.find all('meta',attrs = {'name':'description'}):\n",
    "    proparty_json['Detail_short'] = meta['content'].strip()\n",
    "    \n",
    "for div in soup.find all('div',attrs={'class':'character-count-truncated'})\n",
    "    proparty_json['Detail_Broad']['Description']=div.text.strip()\n",
    "    \n",
    "for (i.script) in enumerate (soup.find all('script',attrs = {'type':'application/id+json'}))\n",
    "    if i==0\n",
    "proparty_json['Detail_Broad']['House name']= json_data['House name']\n",
    "\n",
    "proparty_json['House_name']['Locality']= json_data['address']['addresslocality']\n",
    "\n",
    "proparty_json['House_name']['Area']= json_data['address']['areaaddress']\n",
    "\n",
    "proparty_json['House_name']['amount']= json_data['price']['areaprice']\n",
    "\n",
    "json_data = json.loads(script.text) \n",
    "proparty_json['price in $']= json_data['offers']['price']\n",
    "\n",
    "proparty_json['image']=json_data[image]\n",
    "     break\n",
    "    with open('data.json','w')as outfile:\n",
    "        \n",
    "json.dump(proparty_json,outfile,indent=4)\n",
    "    with open('output_file.html','wb')as file:\n",
    "    file.write(html) \n",
    "    \n",
    "    {\"Details_Broad\":{\"Toatal_house\":4},\n",
    "    \"House_name\":{\"area\":\"satara parisar\",\"Locality\":\"aurangabad\",\"amount\":\"3500\"},\n",
    "     \"House_name\":{\"area\":\"peshave nagar\",\"Locality\":\"aurangabad\",\"amount\":\"350\"},\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Write a python program to scrape mentioned details from ‘https://www.dineout.co.in/delhi-restaurants/buffet\u0002special’ :\n",
    "i) Restaurant name\n",
    "ii) Cuisine\n",
    "iii) Location\n",
    "iv) Ratings\n",
    "v) Image url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'}\n",
    "response = requests.get(\"https://www.dineout.co.in/\",headers=headers)\n",
    "\n",
    "content = response.content\n",
    "soup = BeautifulSoup(content,\"html.parser\")\n",
    "\n",
    "top_rest = soup.find_all(\"div\",attrs={\"class\": \"bb0 collections-grid col-l-16\"})\n",
    "list_tr = top_rest[0].find_all(\"div\",attrs={\"class\": \"col-s-8 col-l-1by3\"})\n",
    "\n",
    "list_rest =[]\n",
    "for tr in list_tr:\n",
    "    dataframe ={}\n",
    "    dataframe[\"rest_name\"] = (tr.find(\"div\",attrs={\"class\": \"res_title zblack bold nowrap\"})).text.replace('\\n', ' ')\n",
    "    dataframe[\"rest_location\"] = (tr.find(\"div\",attrs={\"class\": \"nowrap grey-text fontsize5 ttupper\"})).text.replace('\\n', ' ')\n",
    "    dataframe[\"cuisine_type\"] = (tr.find(\"div\",attrs={\"class\":\"nowrap grey-text\"})).text.replace('\\n', ' ')\n",
    "     dataframe[\"rating_type\"] = (tr.find(\"div\",attrs={\"class\":\"nowrap grey-text\"})).text.replace('\\n', ' ')\n",
    "    list_rest.append(dataframe)\n",
    "list_rest\n",
    "\n",
    "import pandas\n",
    "df = pandas.DataFrame(list_rest)\n",
    "df.to_csv(\"dineout_res.csv\",index=False)\n",
    "print df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) Write a python program to scrape weather details for last 24 hours from\n",
    "‘https://en.tutiempo.net/delhi.html?data=last-24- hours’ :\n",
    "i) Hour\n",
    "ii) Temperature\n",
    "iii) Wind\n",
    "iv) Weather condition\n",
    "v) Humidity\n",
    "vi) Pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " import requests\n",
    "from pprint import pprint\n",
    "\n",
    "def weather_data(query):\n",
    "     res=requests.get('https://en.tutiempo.net/delhi.html?data=last-24-'+query+'&APPID=****************************8&units=metric');\n",
    "         return res.json();\n",
    "\n",
    "def print_weather(result,city):\n",
    "    print(\"{}'s temperature: {}°C \".format(city,result['main']['temp']))\n",
    "    print(\"Wind speed: {} m/s\".format(result['wind']['speed']))\n",
    "    \n",
    "    print(\"Description: {}\".format(result['weather'][0]['description']))\n",
    "    print(\"Weather: {}\".format(result['weather'][0]['main']))\n",
    "    \n",
    "def main():\n",
    "    \n",
    "    city=input('Enter the city:')\n",
    "     print()\n",
    "         try:\n",
    "        query='q='+city;\n",
    "        w_data=weather_data(query);\n",
    "        print_weather(w_data, city)\n",
    "        print()\n",
    "        \n",
    "        except:\n",
    "        print('City name not found...')\n",
    "if __name__=='__main__':\n",
    "     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Write a python program to scrape monument name, monument description, image url about top 10 monuments\n",
    "from 'https://www.puredestinations.co.uk/top-10-famous-monuments-to-visit-in-india/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.puredestinations.co.uk/top-10-famous-monuments-to-visit-in-india/'\n",
    "response = requests.get(url)\n",
    "Soap = BeautifulSoap(response.text,'lxml')\n",
    "\n",
    "Monument = Soap.select('td.rankings-block__container .table-body__cell.name a ')\n",
    "link = [a.attrs.get('href') for a in Soap.select('td.rankings-block__container .table-body__cell.name a ') ]\n",
    "\n",
    "Rating = [b.attrs.get('data-value')\n",
    "              for b in Soap.select('td.rankings-block__container .table .rating')]\n",
    "\n",
    "for index in range(10, len(Monument)):\n",
    "    Manument_string = Manument[index].get_text()\n",
    "    Manument_description = (' ' .join(Manument_string.split()).replace(' . ', ''))\n",
    "    Monument_name = Monument[len(str(index))+1:-7]\n",
    "    Image_url = htmldata = urlopen('https://www.puredestination.co.uk.png/')\n",
    "    soup = BeautifulSoup(htmldata, 'html.parser')\n",
    "    images = soup.find_all('img')\n",
    "  \n",
    "   for item in images:\n",
    "    print(item['src'])\n",
    "    data = [[\"Manument_name\":Manument_name],\n",
    "           [\"Manument_description\": Manument_description], \n",
    "           [\"Image_url\": Image_url[index]],\n",
    "           [\"Link\": link[index]]]\n",
    "    \n",
    "    df = pd.DataFrame(data,colume,colume1 =['Manument_name','Manument_description','Image_url',])\n",
    "    \n",
    "    df "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
